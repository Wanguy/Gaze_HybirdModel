{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import resnet\n",
    "import model\n",
    "\n",
    "maps = 32\n",
    "dim_feature = 7 * 7\n",
    "basenet = resnet.resnet18(True, maps=maps)\n",
    "# lstm = nn.LSTM(dim_feature, dim_feature, 2, bidirectional=True, batch_first=True)\n",
    "lstm = nn.LSTM(maps * dim_feature, maps * dim_feature, 3, bidirectional=False, batch_first=True)\n",
    "linear_after_lstm = nn.Linear(maps * dim_feature, maps * dim_feature)\n",
    "\n",
    "input = torch.randn(4,3,224,224)\n",
    "feature = basenet(input)\n",
    "print(feature.size())\n",
    "\n",
    "net = model.Model()\n",
    "# gaze = net(input)\n",
    "# print(gaze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([4, 1568])\n"
     ]
    }
   ],
   "source": [
    "batch_size = feature.size(0)\n",
    "feature = feature.flatten(1)\n",
    "\n",
    "feature = linear_after_lstm(feature)\n",
    "\n",
    "print(batch_size)\n",
    "print(feature.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([10, 4, 1568])\n"
     ]
    }
   ],
   "source": [
    "feature = torch.randn(10,4,1568)\n",
    "lstm_feature, _ = lstm(feature)\n",
    "print(len(_))\n",
    "# print(_)\n",
    "print(lstm_feature.size())\n",
    "\n",
    "# print(lstm_feature[-1, :])\n",
    "# lstm_feature = lstm_feature[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 1568])\n",
      "torch.Size([10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "d_model = maps * dim_feature\n",
    "nhead = 8\n",
    "dropout = 0.1\n",
    "self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout,batch_first=True)\n",
    "\n",
    "q=k=lstm_feature\n",
    "src2, weight = self_attn(q,k,lstm_feature)\n",
    "print(src2.size())\n",
    "print(weight.size())\n",
    "# print(src2)\n",
    "# print(weight)\n",
    "# print(weight[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64])\n",
      "tensor([[[ 0.0127,  0.2547, -0.5900, -0.5150,  0.8341,  0.3086, -1.2778,\n",
      "           1.0204, -0.1114,  0.6874, -0.4209,  0.2231,  0.0132, -0.7971,\n",
      "          -1.0226,  0.4232, -0.7172, -0.0988,  1.1001, -0.0039,  0.2722,\n",
      "           0.6263, -0.5229, -0.9726,  0.1074, -1.3913,  0.1393, -1.0538,\n",
      "          -0.7352, -0.3824,  0.6050, -0.7491,  0.0127,  0.2547, -0.5900,\n",
      "          -0.5150,  0.8341,  0.3086, -1.2778,  1.0204, -0.1114,  0.6874,\n",
      "          -0.4209,  0.2231,  0.0132, -0.7971, -1.0226,  0.4232, -0.7172,\n",
      "          -0.0988,  1.1001, -0.0039,  0.2722,  0.6263, -0.5229, -0.9726,\n",
      "           0.1074, -1.3913,  0.1393, -1.0538, -0.7352, -0.3824,  0.6050,\n",
      "          -0.7491],\n",
      "         [ 0.0127,  0.2547, -0.5900, -0.5150,  0.8341,  0.3086, -1.2778,\n",
      "           1.0204, -0.1114,  0.6874, -0.4209,  0.2231,  0.0132, -0.7971,\n",
      "          -1.0226,  0.4232, -0.7172, -0.0988,  1.1001, -0.0039,  0.2722,\n",
      "           0.6263, -0.5229, -0.9726,  0.1074, -1.3913,  0.1393, -1.0538,\n",
      "          -0.7352, -0.3824,  0.6050, -0.7491,  0.0127,  0.2547, -0.5900,\n",
      "          -0.5150,  0.8341,  0.3086, -1.2778,  1.0204, -0.1114,  0.6874,\n",
      "          -0.4209,  0.2231,  0.0132, -0.7971, -1.0226,  0.4232, -0.7172,\n",
      "          -0.0988,  1.1001, -0.0039,  0.2722,  0.6263, -0.5229, -0.9726,\n",
      "           0.1074, -1.3913,  0.1393, -1.0538, -0.7352, -0.3824,  0.6050,\n",
      "          -0.7491],\n",
      "         [ 0.0127,  0.2547, -0.5900, -0.5150,  0.8341,  0.3086, -1.2778,\n",
      "           1.0204, -0.1114,  0.6874, -0.4209,  0.2231,  0.0132, -0.7971,\n",
      "          -1.0226,  0.4232, -0.7172, -0.0988,  1.1001, -0.0039,  0.2722,\n",
      "           0.6263, -0.5229, -0.9726,  0.1074, -1.3913,  0.1393, -1.0538,\n",
      "          -0.7352, -0.3824,  0.6050, -0.7491,  0.0127,  0.2547, -0.5900,\n",
      "          -0.5150,  0.8341,  0.3086, -1.2778,  1.0204, -0.1114,  0.6874,\n",
      "          -0.4209,  0.2231,  0.0132, -0.7971, -1.0226,  0.4232, -0.7172,\n",
      "          -0.0988,  1.1001, -0.0039,  0.2722,  0.6263, -0.5229, -0.9726,\n",
      "           0.1074, -1.3913,  0.1393, -1.0538, -0.7352, -0.3824,  0.6050,\n",
      "          -0.7491]]], grad_fn=<RepeatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cls_token = nn.Parameter(torch.randn(1, 1, 32))\n",
    "cls = cls_token.repeat((1, 3, 2))\n",
    "print(cls.size())\n",
    "print(cls)\n",
    "\n",
    "\n",
    "\n",
    "# tr_feature = feature.permute(2, 0, 1)\n",
    "\n",
    "# print(tr_feature.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.8466,  0.3534, -0.7178],\n",
      "        [ 0.0724, -0.3986,  1.7843]], requires_grad=True)\n",
      "tensor([[-0.8466,  0.3534, -0.7178],\n",
      "        [ 0.0724, -0.3986,  1.7843],\n",
      "        [-0.8466,  0.3534, -0.7178],\n",
      "        [ 0.0724, -0.3986,  1.7843],\n",
      "        [-0.8466,  0.3534, -0.7178],\n",
      "        [ 0.0724, -0.3986,  1.7843]], grad_fn=<CatBackward0>)\n",
      "tensor([[-0.8466,  0.3534, -0.7178, -0.8466,  0.3534, -0.7178, -0.8466,  0.3534,\n",
      "         -0.7178],\n",
      "        [ 0.0724, -0.3986,  1.7843,  0.0724, -0.3986,  1.7843,  0.0724, -0.3986,\n",
      "          1.7843]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = nn.Parameter(torch.randn(2,3))\n",
    "print(x)\n",
    "print(torch.cat((x,x,x),0))\n",
    "print(torch.cat((x,x,x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = model.TransformerEncoderLayer(maps*4, nhead, 512, dropout)\n",
    "\n",
    "encoder_norm = nn.LayerNorm(maps*4)\n",
    "# num_encoder_layer: deeps of layers\n",
    "\n",
    "encoder = model.TransformerEncoder(encoder_layer, 6, encoder_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================>> (Begin) Training params << =======================\n",
      "{\n",
      "    \"params\": {\n",
      "        \"batch_size\": 100,\n",
      "        \"epoch\": 100,\n",
      "        \"lr\": 0.0001,\n",
      "        \"decay\": 0.5,\n",
      "        \"decay_step\": 5,\n",
      "        \"warmup\": 0\n",
      "    },\n",
      "    \"save\": {\n",
      "        \"metapath\": \"/home/zjx/Gaze/Output/HybirdGaze\",\n",
      "        \"folder\": \"gaze360_10000\",\n",
      "        \"model_name\": \"trans6\",\n",
      "        \"step\": 5\n",
      "    },\n",
      "    \"data\": {\n",
      "        \"image\": \"/home/zjx/Gaze/Datasets/gaze360_processed_1/Image\",\n",
      "        \"label\": \"/home/zjx/Gaze/Datasets/gaze360_processed_1/Label/train.label\",\n",
      "        \"header\": true,\n",
      "        \"name\": \"gaze360\",\n",
      "        \"isFolder\": false\n",
      "    },\n",
      "    \"pretrain\": {\n",
      "        \"enable\": false,\n",
      "        \"path\": \"PLACEHOLDER\",\n",
      "        \"device\": \"PLACEHOLDER\"\n",
      "    },\n",
      "    \"device\": 1,\n",
      "    \"ave_loss\": 1000,\n",
      "    \"reader\": \"reader\",\n",
      "    \"person\": null\n",
      "}\n",
      "=====================>> (End) Training params << =======================\n",
      "========================> Read Data <========================\n",
      "-- [Read Data]: Source: /home/zjx/Gaze/Datasets/gaze360_processed_1/Label/train.label\n",
      "-- [Read Data]: Total num: 631\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict as edict\n",
    "import yaml\n",
    "import ctools\n",
    "import importlib\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "config = \"./config/config_gaze360.yaml\"\n",
    "\n",
    "person = None\n",
    "\n",
    "config = edict(yaml.load(open(config), Loader=yaml.FullLoader))\n",
    "\n",
    "config = config.train\n",
    "\n",
    "config.person = person\n",
    "\n",
    "config.device = 1\n",
    "\n",
    "print(\"=====================>> (Begin) Training params << =======================\")\n",
    "\n",
    "print(ctools.DictDumps(config))\n",
    "\n",
    "print(\"=====================>> (End) Training params << =======================\")\n",
    "\n",
    "dataloader = importlib.import_module(\"reader.\" + config.reader)\n",
    "device = torch.device(config.device)\n",
    "torch.cuda.set_device(device)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "data = config.data\n",
    "save = config.save\n",
    "params = config.params\n",
    "average_loss_num = config.ave_loss\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "print(\"========================> Read Data <========================\")\n",
    "if config.person is not None:\n",
    "    if data.isFolder:\n",
    "        data, folder = ctools.readfolder(data, [config.person], reverse=True)\n",
    "\n",
    "    save_name = folder[config.person]\n",
    "\n",
    "    dataset = dataloader.loader(data, params.batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "    save_path = os.path.join(save.metapath, save.folder, f\"checkpoint/{save_name}\")\n",
    "\n",
    "else:\n",
    "    if data.isFolder:\n",
    "        data, _ = ctools.readfolder(data)\n",
    "\n",
    "    dataset = dataloader.loader(data, params.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    save_path = os.path.join(save.metapath, save.folder, \"checkpoint\")\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f558825f310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4, 3, 224, 224])\n",
      "100\n",
      "rec_000/head/000015/000003.jpg\n",
      "rec_000/head/000015/000004.jpg\n",
      "rec_000/head/000015/000005.jpg\n",
      "rec_000/head/000015/000006.jpg\n",
      "rec_000/head/000015/000007.jpg\n",
      "rec_000/head/000015/000008.jpg\n",
      "rec_000/head/000015/000009.jpg\n",
      "rec_000/head/000052/000009.jpg\n",
      "rec_000/head/000015/000010.jpg\n",
      "rec_000/head/000015/000011.jpg\n",
      "rec_000/head/000015/000012.jpg\n",
      "rec_000/head/000015/000013.jpg\n",
      "rec_000/head/000015/000014.jpg\n",
      "rec_000/head/000015/000015.jpg\n",
      "rec_000/head/000015/000016.jpg\n",
      "rec_000/head/000015/000017.jpg\n",
      "rec_000/head/000015/000018.jpg\n",
      "rec_000/head/000015/000019.jpg\n",
      "rec_000/head/000015/000020.jpg\n",
      "rec_000/head/000015/000021.jpg\n",
      "rec_000/head/000015/000022.jpg\n",
      "rec_000/head/000015/000023.jpg\n",
      "rec_000/head/000015/000024.jpg\n",
      "rec_000/head/000015/000025.jpg\n",
      "rec_000/head/000015/000026.jpg\n",
      "rec_000/head/000015/000027.jpg\n",
      "rec_000/head/000015/000028.jpg\n",
      "rec_000/head/000015/000029.jpg\n",
      "rec_000/head/000015/000030.jpg\n",
      "rec_000/head/000015/000031.jpg\n",
      "rec_000/head/000015/000032.jpg\n",
      "rec_000/head/000015/000033.jpg\n",
      "rec_000/head/000015/000034.jpg\n",
      "rec_000/head/000015/000035.jpg\n",
      "rec_000/head/000015/000036.jpg\n",
      "rec_000/head/000015/000037.jpg\n",
      "rec_000/head/000015/000038.jpg\n",
      "rec_000/head/000015/000039.jpg\n",
      "rec_000/head/000015/000040.jpg\n",
      "rec_000/head/000015/000041.jpg\n",
      "rec_000/head/000015/000042.jpg\n",
      "rec_000/head/000015/000043.jpg\n",
      "rec_000/head/000015/000044.jpg\n",
      "rec_000/head/000015/000045.jpg\n",
      "rec_000/head/000015/000046.jpg\n",
      "rec_000/head/000015/000047.jpg\n",
      "rec_000/head/000015/000048.jpg\n",
      "rec_000/head/000015/000049.jpg\n",
      "rec_000/head/000015/000050.jpg\n",
      "rec_000/head/000015/000051.jpg\n",
      "rec_000/head/000015/000052.jpg\n",
      "rec_000/head/000015/000053.jpg\n",
      "rec_000/head/000015/000054.jpg\n",
      "rec_000/head/000015/000055.jpg\n",
      "rec_000/head/000016/000094.jpg\n",
      "rec_000/head/000016/000095.jpg\n",
      "rec_000/head/000016/000096.jpg\n",
      "rec_000/head/000016/000097.jpg\n",
      "rec_000/head/000016/000098.jpg\n",
      "rec_000/head/000016/000099.jpg\n",
      "rec_000/head/000016/000110.jpg\n",
      "rec_000/head/000016/000120.jpg\n",
      "rec_000/head/000016/000121.jpg\n",
      "rec_000/head/000016/000122.jpg\n",
      "rec_000/head/000016/000123.jpg\n",
      "rec_000/head/000016/000124.jpg\n",
      "rec_000/head/000016/000125.jpg\n",
      "rec_000/head/000016/000126.jpg\n",
      "rec_000/head/000016/000127.jpg\n",
      "rec_000/head/000016/000128.jpg\n",
      "rec_000/head/000016/000129.jpg\n",
      "rec_000/head/000016/000130.jpg\n",
      "rec_000/head/000016/000131.jpg\n",
      "rec_000/head/000016/000132.jpg\n",
      "rec_000/head/000016/000133.jpg\n",
      "rec_000/head/000016/000134.jpg\n",
      "rec_000/head/000016/000135.jpg\n",
      "rec_000/head/000016/000136.jpg\n",
      "rec_000/head/000016/000137.jpg\n",
      "rec_000/head/000016/000138.jpg\n",
      "rec_000/head/000016/000139.jpg\n",
      "rec_000/head/000016/000140.jpg\n",
      "rec_000/head/000016/000141.jpg\n",
      "rec_000/head/000016/000142.jpg\n",
      "rec_000/head/000016/000143.jpg\n",
      "rec_000/head/000016/000144.jpg\n",
      "rec_000/head/000016/000145.jpg\n",
      "rec_000/head/000016/000146.jpg\n",
      "rec_000/head/000016/000147.jpg\n",
      "rec_000/head/000016/000148.jpg\n",
      "rec_000/head/000016/000149.jpg\n",
      "rec_000/head/000016/000150.jpg\n",
      "rec_000/head/000016/000151.jpg\n",
      "rec_000/head/000016/000152.jpg\n",
      "rec_000/head/000016/000153.jpg\n",
      "rec_000/head/000016/000154.jpg\n",
      "rec_000/head/000016/000155.jpg\n",
      "rec_000/head/000016/000156.jpg\n",
      "rec_000/head/000016/000157.jpg\n",
      "rec_000/head/000016/000158.jpg\n",
      "torch.Size([100, 2])\n",
      "tensor([[ 0.3571, -0.1246],\n",
      "        [ 0.3582, -0.1265],\n",
      "        [ 0.3580, -0.1253],\n",
      "        [ 0.3591, -0.1248],\n",
      "        [ 0.3531, -0.1242],\n",
      "        [ 0.3526, -0.1257],\n",
      "        [ 0.3589, -0.1264],\n",
      "        [-0.1084, -0.1599],\n",
      "        [ 0.3514, -0.1233],\n",
      "        [ 0.3517, -0.1253],\n",
      "        [ 0.3515, -0.1232],\n",
      "        [ 0.3522, -0.1256],\n",
      "        [ 0.3530, -0.1256],\n",
      "        [ 0.3617, -0.1273],\n",
      "        [ 0.3585, -0.1271],\n",
      "        [ 0.3642, -0.1268],\n",
      "        [ 0.3703, -0.1270],\n",
      "        [ 0.3747, -0.1284],\n",
      "        [ 0.3863, -0.1313],\n",
      "        [ 0.3947, -0.1312],\n",
      "        [ 0.4080, -0.1353],\n",
      "        [ 0.4182, -0.1370],\n",
      "        [ 0.4242, -0.1359],\n",
      "        [ 0.4352, -0.1379],\n",
      "        [ 0.4425, -0.1375],\n",
      "        [ 0.4461, -0.1379],\n",
      "        [ 0.4511, -0.1414],\n",
      "        [ 0.4478, -0.1367],\n",
      "        [ 0.4537, -0.1410],\n",
      "        [ 0.4579, -0.1403],\n",
      "        [ 0.4696, -0.1433],\n",
      "        [ 0.4805, -0.1439],\n",
      "        [ 0.5009, -0.1437],\n",
      "        [ 0.5281, -0.1495],\n",
      "        [ 0.5599, -0.1528],\n",
      "        [ 0.6037, -0.1534],\n",
      "        [ 0.6400, -0.1568],\n",
      "        [ 0.6713, -0.1618],\n",
      "        [ 0.7130, -0.1681],\n",
      "        [ 0.7262, -0.1706],\n",
      "        [ 0.7355, -0.1723],\n",
      "        [ 0.7562, -0.1728],\n",
      "        [ 0.7696, -0.1740],\n",
      "        [ 0.7708, -0.1726],\n",
      "        [ 0.7825, -0.1740],\n",
      "        [ 0.8034, -0.1762],\n",
      "        [ 0.8266, -0.1783],\n",
      "        [ 0.8573, -0.1826],\n",
      "        [ 0.9169, -0.1921],\n",
      "        [ 0.9543, -0.1970],\n",
      "        [ 1.0023, -0.2027],\n",
      "        [ 1.0596, -0.2122],\n",
      "        [ 1.0721, -0.2152],\n",
      "        [ 1.1286, -0.2231],\n",
      "        [ 0.1939, -0.1336],\n",
      "        [ 0.2025, -0.1338],\n",
      "        [ 0.2020, -0.1349],\n",
      "        [ 0.1958, -0.1299],\n",
      "        [ 0.1897, -0.1326],\n",
      "        [ 0.1768, -0.1331],\n",
      "        [ 0.1742, -0.1293],\n",
      "        [ 0.3145, -0.1398],\n",
      "        [ 0.3236, -0.1339],\n",
      "        [ 0.3416, -0.1386],\n",
      "        [ 0.3707, -0.1457],\n",
      "        [ 0.3815, -0.1397],\n",
      "        [ 0.4062, -0.1402],\n",
      "        [ 0.4317, -0.1406],\n",
      "        [ 0.4581, -0.1425],\n",
      "        [ 0.4641, -0.1439],\n",
      "        [ 0.4697, -0.1448],\n",
      "        [ 0.4704, -0.1415],\n",
      "        [ 0.4679, -0.1409],\n",
      "        [ 0.4786, -0.1459],\n",
      "        [ 0.4745, -0.1432],\n",
      "        [ 0.4758, -0.1473],\n",
      "        [ 0.4836, -0.1441],\n",
      "        [ 0.4726, -0.1417],\n",
      "        [ 0.4764, -0.1456],\n",
      "        [ 0.4742, -0.1457],\n",
      "        [ 0.4905, -0.1492],\n",
      "        [ 0.5113, -0.1533],\n",
      "        [ 0.5354, -0.1551],\n",
      "        [ 0.5785, -0.1587],\n",
      "        [ 0.6047, -0.1589],\n",
      "        [ 0.6368, -0.1614],\n",
      "        [ 0.6606, -0.1607],\n",
      "        [ 0.6791, -0.1632],\n",
      "        [ 0.7039, -0.1668],\n",
      "        [ 0.7300, -0.1709],\n",
      "        [ 0.7439, -0.1737],\n",
      "        [ 0.7704, -0.1747],\n",
      "        [ 0.8314, -0.1832],\n",
      "        [ 0.8443, -0.1840],\n",
      "        [ 0.8458, -0.1813],\n",
      "        [ 0.8596, -0.1830],\n",
      "        [ 0.8869, -0.1865],\n",
      "        [ 0.9122, -0.1916],\n",
      "        [ 0.9400, -0.1939],\n",
      "        [ 0.9877, -0.2020]])\n"
     ]
    }
   ],
   "source": [
    "for data, label in dataset:\n",
    "    break\n",
    "\n",
    "print(data.face.size())\n",
    "print(len(data.name))\n",
    "for name in data.name:\n",
    "    print(name)\n",
    "print(label.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, (data,label) in enumerate(dataset):\n",
    "    print(data.face.size())\n",
    "#     print(data)\n",
    "#     print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model as model\n",
    "\n",
    "net = model.Model()\n",
    "net.train()\n",
    "out = net(data['face'])\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4, 3, 224, 224])\n",
      "torch.Size([400, 3, 224, 224])\n",
      "torch.Size([400, 32, 7, 7])\n",
      "torch.Size([400, 1568])\n",
      "torch.Size([400, 1568])\n",
      "torch.Size([100, 4, 1568])\n"
     ]
    }
   ],
   "source": [
    "from resnet import resnet18\n",
    "\n",
    "input = data['face']\n",
    "print(input.size())\n",
    "batch_size = input.size(0)\n",
    "input = torch.flatten(input, start_dim=0, end_dim=1)\n",
    "print(input.size())\n",
    "base_model = resnet18(True, maps=maps)\n",
    "feature = base_model(input)\n",
    "print(feature.size())\n",
    "# print(feature.size())\n",
    "feature = feature.flatten(1)\n",
    "print(feature.size())\n",
    "linear1 = nn.Linear(maps * dim_feature, maps * dim_feature)\n",
    "feature = linear1(feature)\n",
    "print(feature.size())\n",
    "\n",
    "feature = torch.reshape(feature, (batch_size, feature.size(0)//batch_size, feature.size(1)))\n",
    "print(feature.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "root = \"/home/zjx/Gaze/Datasets/gaze360\"\n",
    "msg = sio.loadmat(os.path.join(root, \"metadata.mat\"))\n",
    "\n",
    "msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reader.reader as dataloader\n",
    "\n",
    "import ctools\n",
    "\n",
    "if data.isFolder:\n",
    "    data, _ = ctools.readfolder(data)\n",
    "\n",
    "dataset = dataloader.loader(data, params.batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "isinstance(a,list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/zjx/Gaze/Datasets/gaze360_processed_1/Label/train.label\") as f:\n",
    "    dataline = f.readlines()\n",
    "\n",
    "print(len(dataline))\n",
    "dataline.pop(0)\n",
    "print(len(dataline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = dataline[4]\n",
    "print(line)\n",
    "line = line.strip().split(\" \")\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [1,2,3]\n",
    "for item in items:\n",
    "    item+=item\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "transforms_ = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "img = cv2.imread(\"/home/zjx/Gaze/Datasets/gaze360_processed_1/Image/test/Face/1004.jpg\")\n",
    "print(img.shape)\n",
    "img = transforms_(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.unsqueeze(0)\n",
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.tensor()\n",
    "imgs.append(img)\n",
    "imgs.append(img)\n",
    "imgs = torch.tensor(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(32)\n",
    "b = torch.reshape(a,(2,2,2,2,2))\n",
    "print(b[1,1])\n",
    "# c = torch.reshape(b,(4,2,2,2))\n",
    "c = torch.flatten(b, start_dim=0, end_dim=1)\n",
    "print(c.size())\n",
    "print(c[3])\n",
    "\n",
    "d = torch.reshape(c,(2,2,2,2,2))\n",
    "print(d[1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.dim()\n",
    "b.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,4,4)\n",
    "print(a)\n",
    "a = a[:,-1,:].unsqueeze(1)\n",
    "print(a.size())\n",
    "print(a)\n",
    "a = a.unsqueeze(1)\n",
    "print(a.size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0783, -1.1302],\n",
      "        [ 0.6067,  0.0837],\n",
      "        [ 0.6113,  0.3518]])\n",
      "tensor([[-0.5985,  1.3614],\n",
      "        [ 0.2129,  0.2008],\n",
      "        [-0.8670, -0.1806]])\n"
     ]
    }
   ],
   "source": [
    "out = torch.randn(3,2)\n",
    "label = torch.randn(3,2)\n",
    "print(out)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5506)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss = loss_func(out,label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaze",
   "language": "python",
   "name": "gaze"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b523a8312e33d8f35c5fa90403338ea869460b7976496c77286027616cffa2c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
